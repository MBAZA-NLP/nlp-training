{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b134d64d",
   "metadata": {},
   "source": [
    "### Welcome to the second module - Intro to NLP & Text pre-processing basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09d987e",
   "metadata": {},
   "source": [
    "## Intro\n",
    "While building machine learning models, data cleaning and preprocessing is a crucial step in order to create a reliable dataset. This improves the quality of your training data.\n",
    "#### Text data needs to be cleaned and encoded to numerical values before giving them to machine learning models, this process of cleaning and encoding is called as Text Preprocessing\n",
    "\n",
    "Today we are going to learn some basic text cleaning steps; \n",
    "* Understanding the data - See what the data is all about. What should be considered for cleaning the data (punctuations, stopwords, whitespace, etc)\n",
    "* Basic cleaning - What parameters need to be considered for cleaning of text data, like lowercasing, removal of punctuations,removal of stopwords. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb97490",
   "metadata": {},
   "source": [
    "#### Importing libraries\n",
    "* pandas for data manipulation and analysis\n",
    "* re for for using regular expression (RE) functions, to check if a particular string matches the given regular expression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0a04ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import re\n",
    "import string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0710a0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "dataset_url = 'https://github.com/MBAZA-NLP/nlp-training/blob/main/data/kinnews_raw_500.csv'\n",
    "data = pd.read_csv(dataset_url)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1126fef2",
   "metadata": {},
   "source": [
    "#### Lower casing\n",
    "* This is one of the basic pre-processing step. This is an important steps to perform as it helps you to convert all the strings into same casing format so that the texts like Lower, lower, and LOWER are considered same. This helps you to reduce the duplication of same word which might be counted as unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09e7307e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we are headed to the second meetup for the week'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content_lower'] = df['content'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e1835c",
   "metadata": {},
   "source": [
    "#### Removal of punctuations\n",
    "This is again a text standardization process that will help to treat 'umva' and 'umva!' in the same way.\n",
    "\n",
    "We also need to carefully choose the list of punctuations to exclude depending on the use case. For example, the `string.punctuation` in python contains the following punctuation symbols \n",
    "\n",
    "* `!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~`\n",
    "\n",
    "Additionally, we can also use regex (regular expressions)\n",
    "* `[^a-zA-Z0-9\\s]` \n",
    "This pattern means \"substitute every character that is not a number, or a character in the range 'a to z' or 'A to Z' with an empty string\".\n",
    "In fact, inserting the special character ^ at the first place of your regex, you will get the negation.\n",
    "\n",
    "\n",
    "* `re.sub(pattern, replacement, text)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6650a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With string.punctuation list\n",
    "# The str.maketrans() method takes three arguments, the first two are empty strings\n",
    "# and the third is the list of punctuations we remove\n",
    "\n",
    "punct_set = string.punctuation \n",
    "def remove_punct(text):\n",
    "    \"\"\"custom function to remove all punctuations\"\"\"\n",
    "    return text.translate(str.maketrans('', '', punct_set))\n",
    "\n",
    "# df[\"text_no_punct\"] = df[\"text\"].apply(lambda text: remove_punct(text))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41589c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## With re.sub function\n",
    "\n",
    "def remove_special_chars(text, remove_digits=True):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ebe1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_noSymbols = remove_special_chars(data, remove_digits=False)\n",
    "data_noSymbols ['content'] = data['content'].apply(remove_special_chars)\n",
    "data_noSymbols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bbeb0a",
   "metadata": {},
   "source": [
    "#### Removal of stopwords\n",
    "Stopwords are the unnecessary words that even if removed the sentiment of the sentence doesnt change.\n",
    "> The stopwords used here are from the KKLTK Python package for Kinyarwanda and Kirundi languages processing. KKLTK currently provides the sets of stopwords for both languages.  \n",
    "###### Example \n",
    "> Umwihariko wa buri karere ku matungo yabazwe kuri Noheli > Umwihariko buri karere matungo yabazwe Noheli (the removed stopwords are \"wa\", \"ku\", \"kuri\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a096bd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of stopwords in Kinyarwamda\n",
    "STOPWORD_KN = {'aba', 'abo', 'aha', 'aho', 'ari', 'ati', 'aya', 'ayo', 'ba', 'baba', 'babo', 'bari', 'be', 'bo', 'bose',\n",
    "           'bw', 'bwa', 'bwo', 'by', 'bya', 'byo', 'cy', 'cya', 'cyo', 'hafi', 'ibi', 'ibyo', 'icyo', 'iki',\n",
    "           'imwe', 'iri', 'iyi', 'iyo', 'izi', 'izo', 'ka', 'ko', 'ku', 'kuri', 'kuva', 'kwa', 'maze', 'mu', 'muri',\n",
    "           'na', 'naho','nawe', 'ngo', 'ni', 'niba', 'nk', 'nka', 'no', 'nta', 'nuko', 'rero', 'rw', 'rwa', 'rwo', 'ry',\n",
    "           'rya','ubu', 'ubwo', 'uko', 'undi', 'uri', 'uwo', 'uyu', 'wa', 'wari', 'we', 'wo', 'ya', 'yabo', 'yari', 'ye',\n",
    "           'yo', 'yose', 'za', 'zo'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0b15d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORD_KN])\n",
    "\n",
    "# df[\"content\"] = df[\"content\"].apply(lambda text: remove_stopwords(text))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9efe654",
   "metadata": {},
   "source": [
    "### Parallel datasets \n",
    "#### `UPCOMING ZINDI HACKATHON ON TEXT PRE-PROCESSING FOR PARALLEL DATA`\n",
    "> #### `PRACTICE EXERCISE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f252e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the datasets\n",
    "Raw_kn = pd.read_csv(\"....\") \n",
    "Raw_en = pd.read_csv(\"....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70295e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the datasets, follow steps above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26c1ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge datasets"
   ]
  }
 ]
